{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ecff7f8",
   "metadata": {},
   "source": [
    "# Pytorch, Pytorch Lightning, Model monitoring, Hyper-parameter optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb438db",
   "metadata": {},
   "source": [
    "Authors:\n",
    "\n",
    "- Andrei Kartavik - 153925\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ae014",
   "metadata": {},
   "source": [
    "### Import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4e7dc5",
   "metadata": {},
   "source": [
    "### Lightning Data Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c68492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path | str = Path(\"./data\"),\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 4,\n",
    "        val_split: int = 5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.val_split = val_split\n",
    "\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        MNIST(root=self.data_dir, train=True, download=True)\n",
    "        MNIST(root=self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == \"fit\":\n",
    "            full_train = MNIST(\n",
    "                root=self.data_dir, train=True, transform=self.transform, download=False\n",
    "            )\n",
    "\n",
    "            train_len = len(full_train) - self.val_split\n",
    "            val_len = self.val_split\n",
    "\n",
    "            self.train_ds, self.val_ds = random_split(\n",
    "                full_train,\n",
    "                [train_len, val_len],\n",
    "                generator=torch.Generator().manual_seed(42),\n",
    "            )\n",
    "        elif stage == \"test\":\n",
    "            self.test_ds = MNIST(\n",
    "                root=self.data_dir,\n",
    "                train=False,\n",
    "                transform=self.transform,\n",
    "                download=False,\n",
    "            )\n",
    "        elif stage == \"predict\":\n",
    "            self.predict_ds = MNIST(\n",
    "                root=self.data_dir,\n",
    "                train=False,\n",
    "                transform=self.transform,\n",
    "                download=False,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.predict_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7252e9",
   "metadata": {},
   "source": [
    "### LightningModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierMNIST(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, lr: float = 1e-3, dropout: float = 0.1, weight_decay: float = 1e-4\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.dropout_p = dropout\n",
    "        self.weight_decay = weight_decay\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_p),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "        self.train_acc = MulticlassAccuracy(num_classes=10)\n",
    "        self.val_acc = MulticlassAccuracy(num_classes=10)\n",
    "        self.test_acc = MulticlassAccuracy(num_classes=10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "    def _shared_step(\n",
    "        self,\n",
    "        batch,\n",
    "        stage: str,\n",
    "        acc_metric: MulticlassAccuracy,\n",
    "    ) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        acc = acc_metric(preds, y)\n",
    "\n",
    "        self.log(f\"{stage}_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(f\"{stage}_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        return self._shared_step(batch, \"train\", self.train_acc)\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> None:\n",
    "        self._shared_step(batch, \"val\", self.val_acc)\n",
    "\n",
    "    def test_step(\n",
    "        self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> None:\n",
    "        self._shared_step(batch, \"test\", self.test_acc)\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, Any]:\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.5, patience=2\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"},\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a099964",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5907c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    batch_size: int = 64\n",
    "    lr: float = 1e-3\n",
    "    dropout: float = 0.1\n",
    "    weight_decay: float = 1e-4\n",
    "    epochs: int = 10\n",
    "    num_workers: int = 4\n",
    "    project: str = \"put-lightning-hw\"\n",
    "    run_name: str = \"baseline\"\n",
    "\n",
    "\n",
    "def build_callbacks():\n",
    "    return [\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "            filename=\"mnist-{epoch:02d}-{val_loss:.4f}\",\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            patience=4,\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "cfg = TrainConfig()\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "dm = MNISTDataModule(batch_size=cfg.batch_size, num_workers=cfg.num_workers)\n",
    "\n",
    "\n",
    "model = ClassifierMNIST(lr=cfg.lr, dropout=cfg.dropout, weight_decay=cfg.weight_decay)\n",
    "\n",
    "wandb_logger = WandbLogger(project=cfg.project, name=cfg.run_name, log_model=True)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=cfg.epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    logger=wandb_logger,\n",
    "    callbacks=build_callbacks(),\n",
    "    log_every_n_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c9d4eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarlsonav\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20251127_224318-fiw8j1jv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karlsonav/put-lightning-hw/runs/fiw8j1jv' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/karlsonav/put-lightning-hw' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karlsonav/put-lightning-hw' target=\"_blank\">https://wandb.ai/karlsonav/put-lightning-hw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karlsonav/put-lightning-hw/runs/fiw8j1jv' target=\"_blank\">https://wandb.ai/karlsonav/put-lightning-hw/runs/fiw8j1jv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type               | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | features   | Sequential         | 18.8 K | train\n",
      "1 | classifier | Sequential         | 402 K  | train\n",
      "2 | train_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc    | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc   | MulticlassAccuracy | 0      | train\n",
      "----------------------------------------------------------\n",
      "421 K     Trainable params\n",
      "0         Non-trainable params\n",
      "421 K     Total params\n",
      "1.687     Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 860/860 [00:10<00:00, 78.27it/s, v_num=j1jv, train_loss_step=0.0168, train_acc_step=1.000, val_loss_step=0.0267, val_acc_step=1.000, val_loss_epoch=0.0368, val_acc_epoch=0.991, train_loss_epoch=0.0078, train_acc_epoch=0.997]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 860/860 [00:10<00:00, 78.24it/s, v_num=j1jv, train_loss_step=0.0168, train_acc_step=1.000, val_loss_step=0.0267, val_acc_step=1.000, val_loss_epoch=0.0368, val_acc_epoch=0.991, train_loss_epoch=0.0078, train_acc_epoch=0.997]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ef964f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ./put-lightning-hw/fiw8j1jv/checkpoints/mnist-epoch=08-val_loss=0.0349.ckpt\n",
      "Loaded model weights from the checkpoint at ./put-lightning-hw/fiw8j1jv/checkpoints/mnist-epoch=08-val_loss=0.0349.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 121.68it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "     test_acc_epoch         0.9923626184463501\n",
      "     test_loss_epoch        0.02674693986773491\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.02674693986773491,\n",
       "  'test_acc_epoch': 0.9923626184463501}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, dm, ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2e768",
   "metadata": {},
   "source": [
    "### HPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1add0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpo_objective(trial: optuna.Trial) -> float:\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "    dm = MNISTDataModule(batch_size=batch_size)\n",
    "    model = ClassifierMNIST(lr=lr, dropout=dropout, weight_decay=weight_decay)\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=cfg.project,\n",
    "        name=f\"optuna-trial-{trial.number}\",\n",
    "        reinit=\"create_new\",\n",
    "    )\n",
    "    wandb_logger = WandbLogger(experiment=run, log_model=False)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=cfg.epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        logger=wandb_logger,\n",
    "        callbacks=build_callbacks(),\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, dm)\n",
    "\n",
    "    val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "    val_acc = trainer.callback_metrics[\"val_acc\"].item()\n",
    "    trial.set_user_attr(\"val_acc\", val_acc)\n",
    "    trial.set_user_attr(\"best_model_path\", trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445b7c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-27 22:45:24,687] A new study created in memory with name: no-name-f3f0c49e-a4b7-4b08-8e51-6730b718dd71\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/andrejkartavik/Desktop/university/master/semester2/MLOps/project1/wandb/run-20251127_224524-f6xkdoad</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karlsonav/put-lightning-hw/runs/f6xkdoad' target=\"_blank\">optuna-trial-0</a></strong> to <a href='https://wandb.ai/karlsonav/put-lightning-hw' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karlsonav/put-lightning-hw' target=\"_blank\">https://wandb.ai/karlsonav/put-lightning-hw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karlsonav/put-lightning-hw/runs/f6xkdoad' target=\"_blank\">https://wandb.ai/karlsonav/put-lightning-hw/runs/f6xkdoad</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name       | Type               | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | features   | Sequential         | 18.8 K | train\n",
      "1 | classifier | Sequential         | 402 K  | train\n",
      "2 | train_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc    | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc   | MulticlassAccuracy | 0      | train\n",
      "----------------------------------------------------------\n",
      "421 K     Trainable params\n",
      "0         Non-trainable params\n",
      "421 K     Total params\n",
      "1.687     Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "[I 2025-11-27 22:46:59,232] Trial 0 finished with value: 0.047170329838991165 and parameters: {'lr': 7.025063562656615e-05, 'dropout': 0.31406987209232373, 'weight_decay': 2.825818195272384e-05, 'batch_size': 64}. Best is trial 0 with value: 0.047170329838991165.\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/andrejkartavik/Desktop/university/master/semester2/MLOps/project1/wandb/run-20251127_224659-74xsov4d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karlsonav/put-lightning-hw/runs/74xsov4d' target=\"_blank\">optuna-trial-1</a></strong> to <a href='https://wandb.ai/karlsonav/put-lightning-hw' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karlsonav/put-lightning-hw' target=\"_blank\">https://wandb.ai/karlsonav/put-lightning-hw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karlsonav/put-lightning-hw/runs/74xsov4d' target=\"_blank\">https://wandb.ai/karlsonav/put-lightning-hw/runs/74xsov4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name       | Type               | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | features   | Sequential         | 18.8 K | train\n",
      "1 | classifier | Sequential         | 402 K  | train\n",
      "2 | train_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc    | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc   | MulticlassAccuracy | 0      | train\n",
      "----------------------------------------------------------\n",
      "421 K     Trainable params\n",
      "0         Non-trainable params\n",
      "421 K     Total params\n",
      "1.687     Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "[I 2025-11-27 22:47:55,231] Trial 1 finished with value: 0.03551575168967247 and parameters: {'lr': 0.0009027058173154408, 'dropout': 0.2286517095997713, 'weight_decay': 2.6764546413633446e-05, 'batch_size': 128}. Best is trial 1 with value: 0.03551575168967247.\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/andrejkartavik/Desktop/university/master/semester2/MLOps/project1/wandb/run-20251127_224755-kfxtjwl4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karlsonav/put-lightning-hw/runs/kfxtjwl4' target=\"_blank\">optuna-trial-2</a></strong> to <a href='https://wandb.ai/karlsonav/put-lightning-hw' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karlsonav/put-lightning-hw' target=\"_blank\">https://wandb.ai/karlsonav/put-lightning-hw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karlsonav/put-lightning-hw/runs/kfxtjwl4' target=\"_blank\">https://wandb.ai/karlsonav/put-lightning-hw/runs/kfxtjwl4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name       | Type               | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | features   | Sequential         | 18.8 K | train\n",
      "1 | classifier | Sequential         | 402 K  | train\n",
      "2 | train_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc    | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc   | MulticlassAccuracy | 0      | train\n",
      "----------------------------------------------------------\n",
      "421 K     Trainable params\n",
      "0         Non-trainable params\n",
      "421 K     Total params\n",
      "1.687     Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "[I 2025-11-27 22:50:40,742] Trial 2 finished with value: 0.03653668984770775 and parameters: {'lr': 0.0008321937568831512, 'dropout': 0.41928998983819565, 'weight_decay': 0.0002198331923627932, 'batch_size': 32}. Best is trial 1 with value: 0.03551575168967247.\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/andrejkartavik/Desktop/university/master/semester2/MLOps/project1/wandb/run-20251127_225040-37f52vvk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karlsonav/put-lightning-hw/runs/37f52vvk' target=\"_blank\">optuna-trial-3</a></strong> to <a href='https://wandb.ai/karlsonav/put-lightning-hw' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karlsonav/put-lightning-hw' target=\"_blank\">https://wandb.ai/karlsonav/put-lightning-hw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karlsonav/put-lightning-hw/runs/37f52vvk' target=\"_blank\">https://wandb.ai/karlsonav/put-lightning-hw/runs/37f52vvk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name       | Type               | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | features   | Sequential         | 18.8 K | train\n",
      "1 | classifier | Sequential         | 402 K  | train\n",
      "2 | train_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc    | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc   | MulticlassAccuracy | 0      | train\n",
      "----------------------------------------------------------\n",
      "421 K     Trainable params\n",
      "0         Non-trainable params\n",
      "421 K     Total params\n",
      "1.687     Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "[I 2025-11-27 22:53:49,421] Trial 3 finished with value: 0.05262177065014839 and parameters: {'lr': 3.2523881187812815e-05, 'dropout': 0.10713012928854138, 'weight_decay': 0.00020324819410084616, 'batch_size': 32}. Best is trial 1 with value: 0.03551575168967247.\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/andrejkartavik/Desktop/university/master/semester2/MLOps/project1/wandb/run-20251127_225349-ztjfaram</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karlsonav/put-lightning-hw/runs/ztjfaram' target=\"_blank\">optuna-trial-4</a></strong> to <a href='https://wandb.ai/karlsonav/put-lightning-hw' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karlsonav/put-lightning-hw' target=\"_blank\">https://wandb.ai/karlsonav/put-lightning-hw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karlsonav/put-lightning-hw/runs/ztjfaram' target=\"_blank\">https://wandb.ai/karlsonav/put-lightning-hw/runs/ztjfaram</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name       | Type               | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | features   | Sequential         | 18.8 K | train\n",
      "1 | classifier | Sequential         | 402 K  | train\n",
      "2 | train_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc    | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc   | MulticlassAccuracy | 0      | train\n",
      "----------------------------------------------------------\n",
      "421 K     Trainable params\n",
      "0         Non-trainable params\n",
      "421 K     Total params\n",
      "1.687     Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "[I 2025-11-27 22:54:43,706] Trial 4 finished with value: 0.03759666532278061 and parameters: {'lr': 0.0006707415243196819, 'dropout': 0.34112442166192264, 'weight_decay': 0.00021399383651116293, 'batch_size': 128}. Best is trial 1 with value: 0.03551575168967247.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(hpo_objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab06dd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val_loss: 0.03551575168967247\n",
      "Best params: {'lr': 0.0009027058173154408, 'dropout': 0.2286517095997713, 'weight_decay': 2.6764546413633446e-05, 'batch_size': 128}\n",
      "Best val_acc: 0.9901406168937683\n"
     ]
    }
   ],
   "source": [
    "print(\"Best val_loss:\", study.best_trial.value)\n",
    "print(\"Best params:\", study.best_trial.params)\n",
    "print(\"Best val_acc:\", study.best_trial.user_attrs[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca3b074",
   "metadata": {},
   "source": [
    "### Comparing Baseline vs HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648dc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Baseline model metrics===\n",
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 127.90it/s]\n",
      "[{'test_loss_epoch': 0.02674693986773491, 'test_acc_epoch': 0.9923626184463501}]\n",
      "\n",
      "===HPO model metrics===\n",
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 113.06it/s]\n",
      "[{'test_loss_epoch': 0.023135846480727196, 'test_acc_epoch': 0.992534875869751}]\n"
     ]
    }
   ],
   "source": [
    "baseline_ckpt = trainer.checkpoint_callback.best_model_path\n",
    "baseline_model = ClassifierMNIST.load_from_checkpoint(baseline_ckpt)\n",
    "\n",
    "hpo_best_trial = study.best_trial\n",
    "hpo_ckpt = hpo_best_trial.user_attrs[\"best_model_path\"]\n",
    "hpo_model: ClassifierMNIST = ClassifierMNIST.load_from_checkpoint(hpo_ckpt)\n",
    "\n",
    "eval_trainer = pl.Trainer(accelerator=\"auto\", devices=\"auto\", logger=False)\n",
    "\n",
    "print(\"===Baseline model metrics===\")\n",
    "baseline_metrics = eval_trainer.test(baseline_model, dm, verbose=False)\n",
    "print(baseline_metrics)\n",
    "\n",
    "print(\"\\n===HPO model metrics===\")\n",
    "hpo_metrics = eval_trainer.test(hpo_model, dm, verbose=False)\n",
    "print(hpo_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1 (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
